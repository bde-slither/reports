\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Paper Title*\\
{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
}

\maketitle

\begin{abstract}
The project is focused on implementing and solving a multi-player clone of the classic Snake game as a OpenAI Gym \cite{sd1} environment. We propose to create the game catered to the Gym environment.  
\end{abstract}

\begin{IEEEkeywords}
deep learning, reinforment learning
\end{IEEEkeywords}

\section{Introduction}
Recently we have seen a rise in controlling agents directly from high-dimensional inputs like vision and speech with deep learning combined with reinforcement learning \cite{sd3} \cite{sd5}. These are steps toward general artificial intelligence and these methods are directly applied to learn self play such as Alpha Go \cite{sd6} Atari \cite{sd3}. It has become evident that such algorithms achieve good performance on difficult problems without problem specific engineering. Also, it has been observed that the strategies are more complex than the environment. With traditional RL the rewards for a step was immediately available, but with games there can be complex strategies as the rewards can be a result of several consecutive steps. These methods use a wide variety of techniques like Markov decision process, discounted future reward, Q-learning \cite{sd5} and Policy gradient \cite{sd4}.\break
This proposal aims at solving a multi-player snake game inspired by slither.io \cite{sd2} where snakes consume food to increase length and kills other snakes. The game will be developed as a OpenAI Gym \cite{sd2} environment which provides an interface between the game and the learning algorithm. It allows developers to focus on the learning environment rather than interacting with the game. It proposes how the agent can be trained using deep learning techniques along with related works and challenges. It also states the success criteria and improvements that can be applied to it. 


\begin{thebibliography}{00}
\bibitem{sd1} G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. OpenAI
Gym. arXiv preprint arXiv:1606.01540, 2016.
\bibitem{sd2} http://slither.io/
\bibitem{sd3} Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., et al. (2013). ``Playing Atari with deep reinforcement learning." Technical report. Deepmind Technologies, arXiv:1312.5602 [cs.LG]
\bibitem{sd4} Bansal, T., Pachocki, J., Sidor, S., Sutskever, I., Mordatch, I.: ``Emergent complexity
via multi-agent competition." CoRR abs/1710.03748 (2017)
\bibitem{sd5} Qicheng Ma, Hadon Nash, ``Solving Multiplayer Games with Reinforcement Learning,"
\bibitem{sd6} Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. ``Mastering the game of go with deep neural networks and tree search," Nature, 529(7587):484â€“489, 2016.
\end{thebibliography}

\end{document}
